{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e2af73",
   "metadata": {},
   "source": [
    "# Technique 1: reduce data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c2d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.appName(\"optimize-spark\").master(\"spark://host.docker.internal:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"3G\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"hdfs://namenode/shared/spark-logs\").\\\n",
    "        config(\"spark.history.fs.logDirectory\", \"hdfs://namenode/shared/spark-logs\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85371b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e685c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plateTypeCountDF = parkViolations.groupBy(\"Plate Type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912d0ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Plate Type#19], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Plate Type#19, 200), ENSURE_REQUIREMENTS, [id=#34]\n",
      "   +- *(1) HashAggregate(keys=[Plate Type#19], functions=[partial_count(1)])\n",
      "      +- FileScan csv [Plate Type#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Plate Type:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.explain() # used to show the plan before execution, in the UI we can only see executed commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72fad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://namenode/output/plate_type_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a839d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|Plate Type|  count|\n",
      "+----------+-------+\n",
      "|       CCK|     47|\n",
      "|       OMO|     63|\n",
      "|       LMB|     89|\n",
      "|       CLG|     16|\n",
      "|       SOS|    246|\n",
      "|       SPC|    688|\n",
      "|       SUP|     71|\n",
      "|       NYA|    198|\n",
      "|       APP|  40347|\n",
      "|       FAR|     49|\n",
      "|       RGL|  11173|\n",
      "|       CHC|    792|\n",
      "|       STA|   1605|\n",
      "|       BOT|     34|\n",
      "|       RGC|    401|\n",
      "|       TRC|  57575|\n",
      "|       AMB|    196|\n",
      "|       COM|4350603|\n",
      "|       BOB|    150|\n",
      "|       HAM|    328|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b95495ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Plate Type#208, 87), REPARTITION_WITH_NUM, [id=#112]\n",
      "+- FileScan csv [Summons Number#205,Plate ID#206,Registration State#207,Plate Type#208,Issue Date#209,Violation Code#210,Vehicle Body Type#211,Vehicle Make#212,Issuing Agency#213,Street Code1#214,Street Code2#215,Street Code3#216,Vehicle Expiration Date#217,Violation Location#218,Violation Precinct#219,Issuer Precinct#220,Issuer Code#221,Issuer Command#222,Issuer Squad#223,Violation Time#224,Time First Observed#225,Violation County#226,Violation In Front Of Or Opposite#227,House Number#228,... 27 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:string,Plate ID:string,Registration State:string,Plate Type:string,Issue Da...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solve\n",
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/\")\n",
    "parkViolationsPlateTypeDF = parkViolations.repartition(87, \"Plate Type\")\n",
    "parkViolationsPlateTypeDF.explain() # you will see a filescan to read data and exchange hashpartition to shuffle and partition based on Plate Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e75943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) HashAggregate(keys=[Plate Type#208], functions=[count(1)])\n",
      "+- *(1) HashAggregate(keys=[Plate Type#208], functions=[partial_count(1)])\n",
      "   +- Exchange hashpartitioning(Plate Type#208, 87), REPARTITION_WITH_NUM, [id=#124]\n",
      "      +- FileScan csv [Plate Type#208] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Plate Type:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF = parkViolationsPlateTypeDF.groupBy(\"Plate Type\").count()\n",
    "plateTypeCountDF.explain() # check the execution plan, you will see the bottom 2 steps are for creating parkViolationsPlateTypeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20bbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_count.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7162dbc",
   "metadata": {},
   "source": [
    "# Technique 2. Use caching, when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23462882",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/\")\n",
    "parkViolationsPlateTypeDF = parkViolations.repartition(87, \"Plate Type\")\n",
    "plateTypeCountDF = parkViolationsPlateTypeDF.groupBy(\"Plate Type\").count()\n",
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_count.csv\")\n",
    "# we also do a average aggregation\n",
    "plateTypeAvgDF = parkViolationsPlateTypeDF.groupBy(\"Plate Type\").avg() # avg is not meaningful here, but used just as an aggregation example\n",
    "plateTypeAvgDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_avg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are redoing the repartition step each time for plateTypeCountDF and plateTypeAvgDF dataframe\n",
    "# We can prevent the second repartition by caching the result of the first repartition, as shown below (clusters cache memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/\")\n",
    "parkViolationsPlateTypeDF = parkViolations.repartition(87, \"Plate Type\")\n",
    "cachedDF = parkViolationsPlateTypeDF.select('Plate Type').cache() # we are caching only the required field of the dataframe in memory to keep cache size small\n",
    "plateTypeCountDF = cachedDF.groupBy(\"Plate Type\").count()\n",
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_count.csv\")\n",
    "# we also do a average aggregation\n",
    "plateTypeAvgDF = cachedDF.groupBy(\"Plate Type\").avg() # avg is not meaningful here, but used just as an aggregation example\n",
    "plateTypeAvgDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_avg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For very large dataframes we can use persist method to save the dataframe using a combination of cache and disk if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313aeb65",
   "metadata": {},
   "source": [
    "# Technique 3. Join strategies - broadcast join and bucketed joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34092bd2",
   "metadata": {},
   "source": [
    "## 3.1. Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5962c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we are broadcasting the dimension table.\n",
    "#  to move the mapping table to each of the node that has the fact tables data in it and preventing the data shuffle of the large dataset\n",
    "# By default the maximum size for a table to be considered for broadcasting is 10MB\n",
    "# This is set using the spark.sql.autoBroadcastJoinThreshold variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets consider a join without broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490bf68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [Summons Number#16, Issue Date#138]\n",
      "+- *(5) SortMergeJoin [plateType#236], [plateType#288], Inner\n",
      "   :- *(2) Sort [plateType#236 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(plateType#236, 200), ENSURE_REQUIREMENTS, [id=#72]\n",
      "   :     +- *(1) Project [Summons Number#16, Plate Type#19 AS plateType#236]\n",
      "   :        +- *(1) Filter (isnotnull(Plate Type#19) AND (Plate Type#19 = COM))\n",
      "   :           +- FileScan csv [Summons Number#16,Plate Type#19] Batched: false, DataFilters: [isnotnull(Plate Type#19), (Plate Type#19 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input/2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Summons Number:string,Plate Type:string>\n",
      "   +- *(4) Sort [plateType#288 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(plateType#288, 200), ENSURE_REQUIREMENTS, [id=#81]\n",
      "         +- *(3) Project [Plate Type#137 AS plateType#288, Issue Date#138]\n",
      "            +- *(3) Filter (isnotnull(Plate Type#137) AND (Plate Type#137 = COM))\n",
      "               +- FileScan csv [Plate Type#137,Issue Date#138] Batched: false, DataFilters: [isnotnull(Plate Type#137), (Plate Type#137 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input/2016.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Plate Type:string,Issue Date:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parkViolations_2015 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2015.csv\")\n",
    "parkViolations_2016 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2016.csv\")\n",
    "\n",
    "parkViolations_2015 = parkViolations_2015.withColumnRenamed(\"Plate Type\", \"plateType\") # simple column rename for easier joins\n",
    "parkViolations_2016 = parkViolations_2016.withColumnRenamed(\"Plate Type\", \"plateType\")\n",
    "\n",
    "parkViolations_2016_COM = parkViolations_2016.filter(parkViolations_2016.plateType == \"COM\")\n",
    "parkViolations_2015_COM = parkViolations_2015.filter(parkViolations_2015.plateType == \"COM\")\n",
    "\n",
    "joinDF = parkViolations_2015_COM.join(parkViolations_2016_COM, parkViolations_2015_COM.plateType ==  parkViolations_2016_COM.plateType, \"inner\").select(parkViolations_2015_COM[\"Summons Number\"], parkViolations_2016_COM[\"Issue Date\"])\n",
    "joinDF.explain() # you will see SortMergeJoin, with exchange for both dataframes, which means involves data shuffle of both dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8dcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below join will take a very long time with the given infrastructure, do not run, unless needed\n",
    "joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://namenode/output/joined_df_without_broadcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to prevent the data shuffle of 2 large datasets, you can optimize your code to enable broadcast join, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [Summons Number#1338, Issue Date#1460]\n",
      "+- *(2) BroadcastHashJoin [plateType#1558], [plateType#1610], Inner, BuildRight, false\n",
      "   :- *(2) Filter isnotnull(plateType#1558)\n",
      "   :  +- InMemoryTableScan [plateType#1558, Summons Number#1338], [isnotnull(plateType#1558)]\n",
      "   :        +- InMemoryRelation [plateType#1558, Summons Number#1338], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :              +- *(2) HashAggregate(keys=[plateType#1558, Summons Number#1338], functions=[])\n",
      "   :                 +- Exchange hashpartitioning(plateType#1558, Summons Number#1338, 200), ENSURE_REQUIREMENTS, [id=#431]\n",
      "   :                    +- *(1) HashAggregate(keys=[plateType#1558, Summons Number#1338], functions=[])\n",
      "   :                       +- *(1) Project [Plate Type#1341 AS plateType#1558, Summons Number#1338]\n",
      "   :                          +- *(1) Filter (isnotnull(Plate Type#1341) AND (Plate Type#1341 = COM))\n",
      "   :                             +- FileScan csv [Summons Number#1338,Plate Type#1341] Batched: false, DataFilters: [isnotnull(Plate Type#1341), (Plate Type#1341 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input/2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Summons Number:string,Plate Type:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#525]\n",
      "      +- *(1) Filter isnotnull(plateType#1610)\n",
      "         +- InMemoryTableScan [plateType#1610, Issue Date#1460], [isnotnull(plateType#1610)]\n",
      "               +- InMemoryRelation [plateType#1610, Issue Date#1460], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(2) HashAggregate(keys=[plateType#1610, Issue Date#1460], functions=[])\n",
      "                        +- Exchange hashpartitioning(plateType#1610, Issue Date#1460, 200), ENSURE_REQUIREMENTS, [id=#456]\n",
      "                           +- *(1) HashAggregate(keys=[plateType#1610, Issue Date#1460], functions=[])\n",
      "                              +- *(1) Project [Plate Type#1459 AS plateType#1610, Issue Date#1460]\n",
      "                                 +- *(1) Filter (isnotnull(Plate Type#1459) AND (Plate Type#1459 = COM))\n",
      "                                    +- FileScan csv [Plate Type#1459,Issue Date#1460] Batched: false, DataFilters: [isnotnull(Plate Type#1459), (Plate Type#1459 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://namenode/input/2016.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Plate Type:string,Issue Date:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parkViolations_2015 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2015.csv\")\n",
    "parkViolations_2016 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2016.csv\")\n",
    "\n",
    "parkViolations_2015 = parkViolations_2015.withColumnRenamed(\"Plate Type\", \"plateType\") # simple column rename for easier joins\n",
    "parkViolations_2016 = parkViolations_2016.withColumnRenamed(\"Plate Type\", \"plateType\")\n",
    "\n",
    "parkViolations_2015_COM = parkViolations_2015.filter(parkViolations_2015.plateType == \"COM\").select(\"plateType\", \"Summons Number\").distinct()\n",
    "parkViolations_2016_COM = parkViolations_2016.filter(parkViolations_2016.plateType == \"COM\").select(\"plateType\", \"Issue Date\").distinct()\n",
    "\n",
    "parkViolations_2015_COM.cache()\n",
    "parkViolations_2016_COM.cache()\n",
    "\n",
    "parkViolations_2015_COM.count() # will cause parkViolations_2015_COM to be cached\n",
    "parkViolations_2016_COM.count() # will cause parkViolations_2016_COM to be cached\n",
    "\n",
    "joinDF = parkViolations_2015_COM.join(parkViolations_2016_COM.hint(\"broadcast\"), parkViolations_2015_COM.plateType ==  parkViolations_2016_COM.plateType, \"inner\").select(parkViolations_2015_COM[\"Summons Number\"], parkViolations_2016_COM[\"Issue Date\"])\n",
    "joinDF.explain() # you will see BroadcastHashJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://namenode/output/joined_df_with_broadcast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec17bad",
   "metadata": {},
   "source": [
    "## 3.2. Bucketed Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic approach would be to repartition one dataframe by the field on which the join is to be performed and then join with the second dataframe, \n",
    "# this would involve data shuffle for the second dataframe at transformation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach would be to use bucketed joins. Bucketing is a technique which you can use to repartition a dataframe based on a field. \n",
    "# If you bucket both the dataframe based on the filed that they are supposed to be joined on, \n",
    "# it will result in both the dataframes having their data chunks to be made available in the same nodes for joins, \n",
    "# because the location of nodes are chosen using the hash of the partition field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48d48284",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2015.csv\")\n",
    "parkViolations_2016 = spark.read.option(\"header\", True).csv(\"hdfs://namenode/input/2016.csv\")\n",
    "\n",
    "new_column_name_list= list(map(lambda x: x.replace(\" \", \"_\"), parkViolations_2015.columns))\n",
    "\n",
    "parkViolations_2015 = parkViolations_2015.toDF(*new_column_name_list)\n",
    "parkViolations_2015 = parkViolations_2015.filter(parkViolations_2015.Plate_Type == \"COM\").filter(parkViolations_2015.Vehicle_Year == \"2001\")\n",
    "parkViolations_2016 = parkViolations_2016.toDF(*new_column_name_list)\n",
    "parkViolations_2016 = parkViolations_2016.filter(parkViolations_2016.Plate_Type == \"COM\").filter(parkViolations_2016.Vehicle_Year == \"2001\")\n",
    "# we filter for COM and 2001 to limit time taken for the join\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # we do this so that Spark does not auto optimize for broadcast join, setting to -1 means disable\n",
    "\n",
    "parkViolations_2015.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2015\")\n",
    "parkViolations_2016.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34969101",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015_tbl = spark.read.table(\"parkViolations_bkt_2015\")\n",
    "parkViolations_2016_tbl = spark.read.table(\"parkViolations_bkt_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcac05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [Summons_Number#2508, Issue_Date#2614]\n",
      "+- *(3) SortMergeJoin [Vehicle_Year#2543, Plate_Type#2511], [Vehicle_Year#2645, Plate_Type#2613], Inner\n",
      "   :- *(1) Sort [Vehicle_Year#2543 ASC NULLS FIRST, Plate_Type#2511 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter (isnotnull(Plate_Type#2511) AND isnotnull(Vehicle_Year#2543))\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet default.parkviolations_bkt_2015[Summons_Number#2508,Plate_Type#2511,Vehicle_Year#2543] Batched: true, DataFilters: [isnotnull(Plate_Type#2511), isnotnull(Vehicle_Year#2543)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/workspace/test/spark-warehouse/parkviolations_bkt_2015], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Summons_Number:string,Plate_Type:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n",
      "   +- *(2) Sort [Vehicle_Year#2645 ASC NULLS FIRST, Plate_Type#2613 ASC NULLS FIRST], false, 0\n",
      "      +- *(2) Filter (isnotnull(Plate_Type#2613) AND isnotnull(Vehicle_Year#2645))\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet default.parkviolations_bkt_2016[Plate_Type#2613,Issue_Date#2614,Vehicle_Year#2645] Batched: true, DataFilters: [isnotnull(Plate_Type#2613), isnotnull(Vehicle_Year#2645)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/workspace/test/spark-warehouse/parkviolations_bkt_2016], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Plate_Type:string,Issue_Date:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF = parkViolations_2015_tbl.join(parkViolations_2016_tbl, (parkViolations_2015_tbl.Plate_Type ==  parkViolations_2016_tbl.Plate_Type) & (parkViolations_2015_tbl.Vehicle_Year ==  parkViolations_2016_tbl.Vehicle_Year) , \"inner\").select(parkViolations_2015_tbl[\"Summons_Number\"], parkViolations_2016_tbl[\"Issue_Date\"])\n",
    "\n",
    "joinDF.explain() # you will see SortMergeJoin, but no exchange, which means no data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fab406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below join will take a while, approx 30min\n",
    "joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/bkt_joined_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
