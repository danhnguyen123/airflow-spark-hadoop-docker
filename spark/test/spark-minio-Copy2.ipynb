{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4eb4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/09/03 09:51:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75/3199136147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyspark-minio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark://spark-master:7077\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.memory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"512m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     getattr(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 )\n\u001b[1;32m    524\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0mjsparkSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1588\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 raise Py4JError(\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.appName(\"pyspark-minio\").master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.endpoint\", \"http://172.18.0.2:9000\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", \"RRv1gXOjum9869TO9PYt\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", \"hpxIFe0ioyrpDhqGt03SM8sV8AYvPP29SmJ62jl8\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61274e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/01 04:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.\\\n",
    "#         builder.\\\n",
    "#         appName(\"pyspark-minio1\").\\\n",
    "#         master(\"spark://spark-master:7077\").\\\n",
    "#         config(\"spark.executor.memory\", \"512m\").\\\n",
    "#         config(\"spark.hadoop.fs.s3a.endpoint\", \"http://172.18.0.2:9000\").\\\n",
    "#         config(\"spark.hadoop.fs.s3a.access.key\", \"iE3prIJV3tWm6t0LNNIu\").\\\n",
    "#         config(\"spark.hadoop.fs.s3a.secret.key\", \"6qg7oYVMAxJkZFI8fVsrOso17XIwTj6cYpzVVBkz\").\\\n",
    "#         config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\").\\\n",
    "#         config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\").\\\n",
    "#         getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa41a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.hadoop.fs.s3a.endpoin\n",
    "# View log and get S3-API: http://xxx.xx.x.x:9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6606ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = spark.sparkContext.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.get(\"spark.hadoop.fs.s3a.endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602dbd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RRv1gXOjum9869TO9PYt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf.get(\"spark.hadoop.fs.s3a.access.key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c662bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/01 04:44:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('s3a://playground/temp.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f79ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------------+-------------------+--------------+------------+------------+-------------------+\n",
      "|             user_id|phone_number| account_number|           campagin|payment_method|success_ekyc|pro_purchase|        accountName|\n",
      "+--------------------+------------+---------------+-------------------+--------------+------------+------------+-------------------+\n",
      "|060a8e05-2ff3-4c9...|   947845900|263704070001341|1. Existing HDB acc|in-app banking|  2022-06-13|  2022-09-08|       LE HONG NGOC|\n",
      "|108882e7-7fc8-459...|   888235816|068704070184503|1. Existing HDB acc|in-app banking|  2022-08-26|  2022-09-08|         LE MY HANH|\n",
      "|12233148-c010-4e4...|   987722954|068704070171437|1. Existing HDB acc|in-app banking|  2022-06-14|  2022-09-10|      BUI VAN GIANG|\n",
      "|1e6acde9-5d69-42c...|   374351851|999990374351851|1. Existing HDB acc|     pro_renew|  2022-08-09|  2022-09-08|    HOANG VAN THIEN|\n",
      "|20ddfb21-1665-4fc...|   868077077|111704070028365|1. Existing HDB acc|in-app banking|  2022-08-20|  2022-09-10|     BUI HONG QUANG|\n",
      "|4668a071-f22e-41d...|   919238636|082704070005545|1. Existing HDB acc|in-app banking|  2022-07-13|  2022-09-08|     LUU PHUC THANH|\n",
      "|5a6132ee-acc8-469...|   917220226|123704070015811|1. Existing HDB acc|in-app banking|  2022-06-24|  2022-09-08|       TO THUY OANH|\n",
      "|5a9b0c42-2d97-43f...|   947128158|068704070173879|1. Existing HDB acc|in-app banking|  2022-08-04|  2022-09-08|      LE DUY PHUONG|\n",
      "|5b66d9c7-6bc6-4a3...|   983215799|068704070187146|1. Existing HDB acc|in-app banking|  2022-09-07|  2022-09-08|      PHAN VAN VIET|\n",
      "|8cc45d67-6405-458...|   909733028|089704070006933|1. Existing HDB acc|in-app banking|  2022-06-28|  2022-09-10|      DAO XUAN TRUC|\n",
      "|b056bc52-3719-40e...|   932080184|089704070006871|1. Existing HDB acc|in-app banking|  2022-07-01|  2022-09-09|        VU THI THUY|\n",
      "|bd6c3d13-f5fb-4f3...|   938638169|014704070007774|1. Existing HDB acc|          cash|  2022-04-29|  2022-09-08| TRAN THI HONG DIEU|\n",
      "|cc2febc2-7be7-415...|   355097069|228704070011511|1. Existing HDB acc|in-app banking|  2022-08-17|  2022-09-09|       DINH VAN NHO|\n",
      "|d0eb4b72-657b-48b...|   876667373|068704070179071|1. Existing HDB acc|in-app banking|  2022-08-16|  2022-09-10|       VO AN H KHOA|\n",
      "|ed0cbf71-1f1e-470...|   345280737|104704070006938|1. Existing HDB acc|in-app banking|  2022-07-06|  2022-09-08|    NGUYEN VAN TOAN|\n",
      "|1dcc548b-a33d-43d...|   963099780|068704070187303|     2. New HDB acc|in-app banking|  2022-09-09|  2022-09-09|     DINH THI LUYEN|\n",
      "|289bf7c8-42f0-4d9...|   904020583|068704070187279|     2. New HDB acc|in-app banking|  2022-09-08|  2022-09-09|LUONG THI ANH TUYET|\n",
      "|7ce9ee07-b59b-47b...|   392219121|068704070187290|     2. New HDB acc|in-app banking|  2022-09-09|  2022-09-09|    NGUYEN THIEN AN|\n",
      "|a998cc49-3224-4be...|   974286156|999990974286156|     2. New HDB acc|in-app banking|  2022-09-09|  2022-09-09|    NGUYEN VAN TUNG|\n",
      "|b1fe84ee-c73d-4a1...|   334074063|068704070187306|     2. New HDB acc|in-app banking|  2022-09-09|  2022-09-10|     LE CANH NGUYEN|\n",
      "+--------------------+------------+---------------+-------------------+--------------+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f94729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe stored in Hadoop.\n"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"success_ekyc\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://namenode/ekyc\")\n",
    "print(\"Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a492bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe stored in Hadoop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"success_ekyc\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://172.18.0.10:8020/ekyctest\")\n",
    "print(\"Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a17b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"hdfs://172.18.0.10:8020/ekyctest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b12da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------------+-------------------+--------------+------------+-----------------+------------+\n",
      "|             user_id|phone_number| account_number|           campagin|payment_method|pro_purchase|      accountName|success_ekyc|\n",
      "+--------------------+------------+---------------+-------------------+--------------+------------+-----------------+------------+\n",
      "|1dcc548b-a33d-43d...|   963099780|068704070187303|     2. New HDB acc|in-app banking|  2022-09-09|   DINH THI LUYEN|  2022-09-09|\n",
      "|7ce9ee07-b59b-47b...|   392219121|068704070187290|     2. New HDB acc|in-app banking|  2022-09-09|  NGUYEN THIEN AN|  2022-09-09|\n",
      "|a998cc49-3224-4be...|   974286156|999990974286156|     2. New HDB acc|in-app banking|  2022-09-09|  NGUYEN VAN TUNG|  2022-09-09|\n",
      "|b1fe84ee-c73d-4a1...|   334074063|068704070187306|     2. New HDB acc|in-app banking|  2022-09-10|   LE CANH NGUYEN|  2022-09-09|\n",
      "|d23a42c7-b8ab-48c...|   969701624|068704070187317|     2. New HDB acc|in-app banking|  2022-09-09|  NGUYEN KY PHONG|  2022-09-09|\n",
      "|ea3bea7e-b776-4e0...|   364763244|999990364763244|     2. New HDB acc|in-app banking|  2022-09-09|TRAN THI NHA LINH|  2022-09-09|\n",
      "|ed0cbf71-1f1e-470...|   345280737|104704070006938|1. Existing HDB acc|in-app banking|  2022-09-08|  NGUYEN VAN TOAN|  2022-07-06|\n",
      "|4668a071-f22e-41d...|   919238636|082704070005545|1. Existing HDB acc|in-app banking|  2022-09-08|   LUU PHUC THANH|  2022-07-13|\n",
      "|20ddfb21-1665-4fc...|   868077077|111704070028365|1. Existing HDB acc|in-app banking|  2022-09-10|   BUI HONG QUANG|  2022-08-20|\n",
      "|5a9b0c42-2d97-43f...|   947128158|068704070173879|1. Existing HDB acc|in-app banking|  2022-09-08|    LE DUY PHUONG|  2022-08-04|\n",
      "|8cc45d67-6405-458...|   909733028|089704070006933|1. Existing HDB acc|in-app banking|  2022-09-10|    DAO XUAN TRUC|  2022-06-28|\n",
      "|5b66d9c7-6bc6-4a3...|   983215799|068704070187146|1. Existing HDB acc|in-app banking|  2022-09-08|    PHAN VAN VIET|  2022-09-07|\n",
      "|12233148-c010-4e4...|   987722954|068704070171437|1. Existing HDB acc|in-app banking|  2022-09-10|    BUI VAN GIANG|  2022-06-14|\n",
      "|5a6132ee-acc8-469...|   917220226|123704070015811|1. Existing HDB acc|in-app banking|  2022-09-08|     TO THUY OANH|  2022-06-24|\n",
      "|d0eb4b72-657b-48b...|   876667373|068704070179071|1. Existing HDB acc|in-app banking|  2022-09-10|     VO AN H KHOA|  2022-08-16|\n",
      "|cc2febc2-7be7-415...|   355097069|228704070011511|1. Existing HDB acc|in-app banking|  2022-09-09|     DINH VAN NHO|  2022-08-17|\n",
      "|060a8e05-2ff3-4c9...|   947845900|263704070001341|1. Existing HDB acc|in-app banking|  2022-09-08|     LE HONG NGOC|  2022-06-13|\n",
      "|b056bc52-3719-40e...|   932080184|089704070006871|1. Existing HDB acc|in-app banking|  2022-09-09|      VU THI THUY|  2022-07-01|\n",
      "|e41b91cc-392f-49f...|   907935586|999991000241000|     2. New HDB acc|in-app banking|  2022-09-10| NGUYEN NGOC MINH|  2022-09-10|\n",
      "|1e6acde9-5d69-42c...|   374351851|999990374351851|1. Existing HDB acc|     pro_renew|  2022-09-08|  HOANG VAN THIEN|  2022-08-09|\n",
      "+--------------------+------------+---------------+-------------------+--------------+------------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
